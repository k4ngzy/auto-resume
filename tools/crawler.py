import asyncio
import csv
import os
import time

from playwright.async_api import TimeoutError as PlaywrightTimeoutError
from playwright.async_api import async_playwright
from tqdm import tqdm


async def get_job_details(url: str, max_count: int = 50, max_retries: int = 3) -> str:
    """
    æŠ“å–èŒä½æœç´¢é¡µé¢ä¸­çš„èŒä½è¯¦æƒ…ï¼ˆä½¿ç”¨Playwrightå¼‚æ­¥å®ç°ï¼Œæ”¯æŒå·²ç™»å½•çŠ¶æ€ï¼‰
    """

    login_data_dir = "./user_data"
    os.makedirs("data", exist_ok=True)
    output_path = f"data/job_details_{int(time.time())}.csv"

    # åˆå§‹åŒ–CSVæ–‡ä»¶
    with open(output_path, "w", encoding="utf-8-sig", newline="") as f:
        csv_writer = csv.DictWriter(
            f,
            fieldnames=[
                "å…¬å¸åç§°",
                "èŒä½åç§°",
                "å·¥ä½œåœ°ç‚¹",
                "è–ªèµ„èŒƒå›´",
                "å·¥ä½œç»éªŒ",
                "å­¦å†è¦æ±‚",
                "èŒä½æ ‡ç­¾",
                "æ‰€éœ€æŠ€èƒ½",
                "å…¬å¸è§„æ¨¡",
                "å…¬å¸é˜¶æ®µ",
                "æ‰€å±è¡Œä¸š",
                "å²—ä½æè¿°",
            ],
        )
        csv_writer.writeheader()

    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        browser = await p.chromium.launch_persistent_context(
            user_data_dir=login_data_dir,
            headless=False,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-setuid-sandbox",
            ],
        )
        browser_page = await browser.new_page()

        captured_responses = []

        # å¼‚æ­¥å“åº”æ‹¦æˆªå™¨
        async def handle_response(response):
            if "job/detail.json" in response.url:
                try:
                    await _parse_response(response, captured_responses)
                except Exception as e:
                    print(f"è§£æå“åº”å¤±è´¥: {e}")

        def sync_handle_response(response):
            asyncio.create_task(handle_response(response))

        browser_page.on("response", sync_handle_response)

        # ç¦ç”¨è‡ªåŠ¨åŒ–ç‰¹å¾
        await browser_page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined })
        """)

        # é¡µé¢åŠ è½½é‡è¯•é€»è¾‘
        for attempt in range(max_retries):
            try:
                await browser_page.goto("https://www.baidu.com", timeout=60000)
                await browser_page.goto(url, timeout=60000, wait_until="domcontentloaded")
                await browser_page.wait_for_selector(".job-info", timeout=30000)
                await browser_page.wait_for_timeout(1000)
                break
            except PlaywrightTimeoutError:
                if attempt == max_retries - 1:
                    raise Exception(f"ç»è¿‡{max_retries}æ¬¡å°è¯•åä»æ— æ³•åŠ è½½é¡µé¢")
                print(f"é¡µé¢åŠ è½½è¶…æ—¶ï¼Œæ­£åœ¨è¿›è¡Œç¬¬{attempt + 2}æ¬¡é‡è¯•...")
                await browser_page.reload(timeout=60000)

        # æ»šåŠ¨åŠ è½½æ›´å¤šå²—ä½
        last_height = await browser_page.evaluate("document.body.scrollHeight")
        for _ in range(3):
            await browser_page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await browser_page.wait_for_timeout(800)
            new_height = await browser_page.evaluate("document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

        # è·å–å²—ä½å¡ç‰‡
        cards = await browser_page.locator(".job-info").all()
        print(f"å‘ç° {len(cards)} ä¸ªå²—ä½å¡ç‰‡")

        count = 0
        max_jobs = min(len(cards), max_count)
        pbar = None
        jobs_buffer = []  # ğŸ”¥ ä½¿ç”¨ç¼“å†²åŒºæ”¶é›†æ•°æ®

        try:
            if max_jobs > 0:
                pbar = tqdm(total=max_jobs, desc="æŠ“å–å²—ä½ä¸­")

            # ğŸ”¥ ä¸ä½¿ç”¨ aiofilesï¼Œå…ˆæ”¶é›†æ‰€æœ‰æ•°æ®
            for card in cards:
                if count >= max_jobs:
                    break

                try:
                    captured_responses.clear()
                    await card.scroll_into_view_if_needed()
                    await card.click()

                    # ç­‰å¾…å“åº”æ•è·
                    wait_time = 0
                    max_wait = 3
                    while len(captured_responses) == 0 and wait_time < max_wait:
                        await browser_page.wait_for_timeout(200)
                        wait_time += 0.2

                    if len(captured_responses) == 0:
                        if pbar:
                            pbar.write("æœªæ•è·åˆ°èŒä½è¯¦æƒ…å“åº”ï¼Œè·³è¿‡è¯¥èŒä½")
                        else:
                            print("æœªæ•è·åˆ°èŒä½è¯¦æƒ…å“åº”ï¼Œè·³è¿‡è¯¥èŒä½")
                        continue

                    # è§£æèŒä½æ•°æ®
                    json_data = captured_responses[0]
                    zp_data = json_data.get("zpData", {})
                    job_info = zp_data.get("jobInfo", {})
                    brand_com_info = zp_data.get("brandComInfo", {})

                    job_data = {
                        "å…¬å¸åç§°": brand_com_info.get("brandName", ""),
                        "èŒä½åç§°": job_info.get("jobName", ""),
                        "å·¥ä½œåœ°ç‚¹": job_info.get("address", ""),
                        "è–ªèµ„èŒƒå›´": job_info.get("salaryDesc", ""),
                        "å·¥ä½œç»éªŒ": job_info.get("jobExperience", "æ— è¦æ±‚"),
                        "å­¦å†è¦æ±‚": job_info.get("degreeName", ""),
                        "èŒä½æ ‡ç­¾": job_info.get("experienceName", ""),
                        "æ‰€éœ€æŠ€èƒ½": ",".join(job_info.get("showSkills", [])),
                        "å…¬å¸è§„æ¨¡": brand_com_info.get("scaleName", ""),
                        "å…¬å¸é˜¶æ®µ": brand_com_info.get("stageName", ""),
                        "æ‰€å±è¡Œä¸š": brand_com_info.get("industryName", ""),
                        "å²—ä½æè¿°": job_info.get("postDescription", "").strip(),
                    }

                    # ğŸ”¥ æ·»åŠ åˆ°ç¼“å†²åŒº
                    jobs_buffer.append(job_data)
                    count += 1

                    if pbar:
                        pbar.update(1)
                        pbar.write(f"âœ… å·²æŠ“å–: {job_data['èŒä½åç§°']} - {job_data['å…¬å¸åç§°']}")

                    await browser_page.wait_for_timeout(500)

                except Exception as e:
                    error_msg = f"å¤„ç†èŒä½æ—¶å‡ºé”™: {str(e)}"
                    if pbar:
                        pbar.write(error_msg)
                    else:
                        print(error_msg)

            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæŠ“å–å®Œæˆåï¼Œä¸€æ¬¡æ€§åŒæ­¥å†™å…¥æ‰€æœ‰æ•°æ®
            if jobs_buffer:
                with open(output_path, "a", encoding="utf-8-sig", newline="") as f:
                    writer = csv.DictWriter(f, fieldnames=jobs_buffer[0].keys())
                    writer.writerows(jobs_buffer)
                print(f"ğŸ’¾ å·²å†™å…¥ {len(jobs_buffer)} æ¡èŒä½æ•°æ®")
            else:
                print("âš ï¸ æœªæŠ“å–åˆ°ä»»ä½•èŒä½æ•°æ®")

        finally:
            # å…³é—­è¿›åº¦æ¡
            if pbar is not None:
                try:
                    pbar.close()  # ğŸ”¥ ä¹Ÿä¸è¦ await
                except Exception:
                    pass
            # ç¡®ä¿æµè§ˆå™¨å…³é—­
            await browser.close()

    print(f"âœ… å·²è·å–èŒä½æ•°æ®,ä¿å­˜è·¯å¾„: {output_path}")
    return output_path


# å¼‚æ­¥è§£æå“åº”çš„è¾…åŠ©å‡½æ•°
async def _parse_response(response, captured_responses):
    try:
        data = await response.json()
        captured_responses.append(data)
    except Exception as e:
        print(f"è§£æå“åº”å¤±è´¥: {e}")


if __name__ == "__main__":
    test_url = "https://www.zhipin.com/web/geek/jobs?city=100010000&position=101310"
    try:
        result = asyncio.run(get_job_details(test_url, max_count=3))
    except Exception as e:
        print(f"æŠ“å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
