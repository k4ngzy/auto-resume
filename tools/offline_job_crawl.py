import argparse
import csv
import json
import os
import sys
import time
from pathlib import Path

from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
from playwright.sync_api import sync_playwright
from tqdm import tqdm

ROOT_DIR = Path(__file__).resolve().parent.parent
sys.path.append(str(ROOT_DIR))


DEFAULT_OUTPUT_DIR = ROOT_DIR / "backend" / "data" / "offline_jobs"
DEFAULT_COMBINED_PATH = ROOT_DIR / "backend" / "data" / "offline_jobs.jsonl"

job_dict = {
    "Java": "100101",
    "C/C++": "100102",
    "Python": "100109",
    "Golang": "100116",
    "Node.js": "100114",
    "å›¾åƒç®—æ³•": "101306",
    "è‡ªç„¶è¯­è¨€å¤„ç†ç®—æ³•": "100117",
    "å¤§æ¨¡å‹ç®—æ³•": "101310",
    "æ•°æ®æŒ–æ˜": "100104",
    "è§„æ§ç®—æ³•": "101311",
    "SLAMç®—æ³•": "101312",
    "æ¨èç®—æ³•": "100118",
    "æœç´¢ç®—æ³•": "100115",
}


def get_job_url(except_job: dict, city="100010000", jobType="1901") -> str:
    """
    æ ¹æ®ç”¨æˆ·æä¾›çš„æ±‚èŒä¿¡æ¯ï¼Œç”ŸæˆèŒä½æœç´¢URL
    Args:
        except_job (dict): åŒ…å«æ±‚èŒä¿¡æ¯çš„å­—å…¸ï¼Œæ ¼å¼ä¸º:
            {
                "job": str,         # èŒä½åç§°ï¼Œå¦‚"å¤§æ¨¡å‹ç®—æ³•å·¥ç¨‹å¸ˆ"
            }
        city (str): åŸå¸‚ç¼–ç ï¼Œé»˜è®¤å…¨å›½
        position (str): èŒä½ç¼–ç ï¼Œé»˜è®¤å¤§æ¨¡å‹ç®—æ³•å²—ä½
        jobType (str): å·¥ä½œç±»å‹ç¼–ç ï¼Œé»˜è®¤å…¨èŒ
    Returns:
        str: ç”Ÿæˆçš„èŒä½æœç´¢URL
    """

    # ä»except_jobå­—å…¸ä¸­æå–å„å­—æ®µï¼Œé»˜è®¤ä¸º"ä¸é™"
    job = except_job.get("job", "")

    position = job_dict.get(job)

    # æ„å»ºåŸºç¡€URL
    url = f"https://www.zhipin.com/web/geek/jobs?city={city}&position={position}&jobType={jobType}"

    return url


def get_job_details(
    url: str,
    max_count: int = 200,
    max_retries: int = 3,
    min_description_length: int = 200,
    output_dir: str = "jobs/101310",
) -> str:
    """
    æŠ“å–èŒä½æœç´¢é¡µé¢ä¸­çš„èŒä½è¯¦æƒ…ï¼ˆä½¿ç”¨PlaywrightåŒæ­¥å®ç°ï¼Œæ”¯æŒå·²ç™»å½•çŠ¶æ€ï¼‰

    Args:
        url: èŒä½æœç´¢é¡µé¢URL
        max_count: éœ€è¦æŠ“å–çš„èŒä½æ•°é‡
        max_retries: é¡µé¢åŠ è½½é‡è¯•æ¬¡æ•°
        min_description_length: å²—ä½æè¿°æœ€å°å­—æ•°ï¼Œå°äºæ­¤å­—æ•°çš„å²—ä½å°†è¢«è¿‡æ»¤
        output_dir: è¾“å‡ºç›®å½•
    """

    login_data_dir = os.getenv("JOB_CRAWL_USER_DATA_DIR", "./user_data")
    os.makedirs(output_dir, exist_ok=True)
    output_path = f"{output_dir}/job_details_{int(time.time())}.csv"

    # åˆå§‹åŒ–CSVæ–‡ä»¶
    with open(output_path, "w", encoding="utf-8-sig", newline="") as f:
        csv_writer = csv.DictWriter(
            f,
            fieldnames=[
                "å…¬å¸åç§°",
                "èŒä½åç§°",
                "å·¥ä½œåœ°ç‚¹",
                "è–ªèµ„èŒƒå›´",
                "å·¥ä½œç»éªŒ",
                "å­¦å†è¦æ±‚",
                "èŒä½æ ‡ç­¾",
                "æ‰€éœ€æŠ€èƒ½",
                "å…¬å¸è§„æ¨¡",
                "å…¬å¸é˜¶æ®µ",
                "æ‰€å±è¡Œä¸š",
                "å²—ä½æè¿°",
            ],
        )
        csv_writer.writeheader()

    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        browser = p.chromium.launch_persistent_context(
            user_data_dir=login_data_dir,
            headless=False,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox",
                "--disable-setuid-sandbox",
            ],
        )
        browser_page = browser.new_page()

        captured_responses = []

        # å“åº”æ‹¦æˆªå™¨
        def handle_response(response):
            if "job/detail.json" in response.url:
                try:
                    _parse_response(response, captured_responses)
                except Exception as e:
                    print(f"è§£æå“åº”å¤±è´¥: {e}")

        browser_page.on("response", handle_response)

        # ç¦ç”¨è‡ªåŠ¨åŒ–ç‰¹å¾
        browser_page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined })
        """)

        # é¡µé¢åŠ è½½é‡è¯•é€»è¾‘
        for attempt in range(max_retries):
            try:
                browser_page.goto("https://www.baidu.com", timeout=60000)
                browser_page.goto(url, timeout=60000, wait_until="domcontentloaded")
                browser_page.wait_for_selector(".job-info", timeout=30000)
                browser_page.wait_for_timeout(1000)
                break
            except PlaywrightTimeoutError:
                if attempt == max_retries - 1:
                    raise Exception(f"ç»è¿‡{max_retries}æ¬¡å°è¯•åä»æ— æ³•åŠ è½½é¡µé¢")
                print(f"é¡µé¢åŠ è½½è¶…æ—¶ï¼Œæ­£åœ¨è¿›è¡Œç¬¬{attempt + 2}æ¬¡é‡è¯•...")
                browser_page.reload(timeout=60000)

        count = 0
        valid_count = 0  # æœ‰æ•ˆå²—ä½è®¡æ•°ï¼ˆæ»¡è¶³å­—æ•°è¦æ±‚ï¼‰
        filtered_count = 0  # è¢«è¿‡æ»¤çš„å²—ä½è®¡æ•°
        filtered_english_count = 0  # è‹±æ–‡JDè¿‡æ»¤è®¡æ•°
        page_num = 1
        pbar = None
        jobs_buffer = []

        try:
            pbar = tqdm(total=max_count, desc="æŠ“å–å²—ä½ä¸­")

            # å¾ªç¯ç¿»é¡µç›´åˆ°æŠ“å–åˆ°è¶³å¤Ÿçš„å²—ä½
            while valid_count < max_count:
                # å…ˆæ»šåŠ¨15æ¬¡åŠ è½½è¶³å¤Ÿçš„å²—ä½ï¼ˆæŒ‰ç…§åŸæ¥crawler.pyçš„é€»è¾‘ï¼‰
                pbar.write(f"ğŸ“„ ç¬¬{page_num}é¡µï¼šæ­£åœ¨æ»šåŠ¨åŠ è½½å²—ä½...")
                last_height = browser_page.evaluate("document.body.scrollHeight")
                for scroll_count in range(5):  # æ»šåŠ¨5æ¬¡
                    browser_page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    browser_page.wait_for_timeout(800)
                    new_height = browser_page.evaluate("document.body.scrollHeight")
                    if new_height == last_height:
                        pbar.write(f"   å·²æ»šåŠ¨{scroll_count + 1}æ¬¡ï¼Œé¡µé¢é«˜åº¦ä¸å†å˜åŒ–")
                        break
                    last_height = new_height
                else:
                    pbar.write("   å·²å®Œæˆ5æ¬¡æ»šåŠ¨")

                # è·å–å²—ä½å¡ç‰‡
                cards = browser_page.locator(".job-info").all()

                if len(cards) == 0:
                    pbar.write(f"âš ï¸ ç¬¬{page_num}é¡µæ²¡æœ‰æ‰¾åˆ°å²—ä½å¡ç‰‡ï¼Œå¯èƒ½å·²åˆ°æœ€åä¸€é¡µ")
                    break

                pbar.write(f"ğŸ“„ ç¬¬{page_num}é¡µå‘ç° {len(cards)} ä¸ªå²—ä½å¡ç‰‡")

                # å¤„ç†å½“å‰é¡µçš„å²—ä½
                for card in cards:
                    if valid_count >= max_count:
                        break

                    try:
                        captured_responses.clear()
                        card.scroll_into_view_if_needed()
                        card.click()

                        # ç­‰å¾…å“åº”æ•è·
                        wait_time = 0
                        max_wait = 3
                        while len(captured_responses) == 0 and wait_time < max_wait:
                            browser_page.wait_for_timeout(200)
                            wait_time += 0.2

                        if len(captured_responses) == 0:
                            pbar.write("âš ï¸ æœªæ•è·åˆ°èŒä½è¯¦æƒ…å“åº”ï¼Œè·³è¿‡è¯¥èŒä½")
                            continue

                        # è§£æèŒä½æ•°æ®
                        json_data = captured_responses[0]
                        zp_data = json_data.get("zpData", {})
                        job_info = zp_data.get("jobInfo", {})
                        brand_com_info = zp_data.get("brandComInfo", {})

                        job_description = job_info.get("postDescription", "").strip()

                        # è¿‡æ»¤1ï¼šå²—ä½æè¿°å­—æ•°å°äºæŒ‡å®šé•¿åº¦çš„è·³è¿‡
                        if len(job_description) < min_description_length:
                            filtered_count += 1
                            pbar.write(
                                f"â­ï¸  è¿‡æ»¤(å­—æ•°): {job_info.get('jobName', '')} - {brand_com_info.get('brandName', '')} "
                                f"(æè¿°ä»…{len(job_description)}å­—ï¼Œå°äº{min_description_length}å­—)"
                            )
                            browser_page.wait_for_timeout(300)
                            continue

                        # è¿‡æ»¤2ï¼šè‹±æ–‡JDï¼ˆåˆ¤æ–­è‹±æ–‡å­—ç¬¦å æ¯”ï¼‰
                        english_chars = sum(1 for c in job_description if c.isascii() and c.isalpha())
                        total_chars = len(job_description)
                        english_ratio = english_chars / total_chars if total_chars > 0 else 0

                        # å¦‚æœè‹±æ–‡å­—ç¬¦å æ¯”è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯è‹±æ–‡JD
                        if english_ratio > 0.3:
                            filtered_english_count += 1
                            pbar.write(
                                f"â­ï¸  è¿‡æ»¤(è‹±æ–‡): {job_info.get('jobName', '')} - {brand_com_info.get('brandName', '')} "
                                f"(è‹±æ–‡å æ¯”{english_ratio:.1%})"
                            )
                            browser_page.wait_for_timeout(300)
                            continue

                        job_data = {
                            "å…¬å¸åç§°": brand_com_info.get("brandName", ""),
                            "èŒä½åç§°": job_info.get("jobName", ""),
                            "å·¥ä½œåœ°ç‚¹": job_info.get("address", ""),
                            "è–ªèµ„èŒƒå›´": job_info.get("salaryDesc", ""),
                            "å·¥ä½œç»éªŒ": job_info.get("jobExperience", "æ— è¦æ±‚"),
                            "å­¦å†è¦æ±‚": job_info.get("degreeName", ""),
                            "èŒä½æ ‡ç­¾": job_info.get("experienceName", ""),
                            "æ‰€éœ€æŠ€èƒ½": ",".join(job_info.get("showSkills", [])),
                            "å…¬å¸è§„æ¨¡": brand_com_info.get("scaleName", ""),
                            "å…¬å¸é˜¶æ®µ": brand_com_info.get("stageName", ""),
                            "æ‰€å±è¡Œä¸š": brand_com_info.get("industryName", ""),
                            "å²—ä½æè¿°": job_description,
                        }

                        # æ·»åŠ åˆ°ç¼“å†²åŒº
                        jobs_buffer.append(job_data)
                        valid_count += 1
                        count += 1

                        pbar.update(1)
                        pbar.write(
                            f"âœ… [{valid_count}/{max_count}] {job_data['èŒä½åç§°']} - {job_data['å…¬å¸åç§°']} "
                            f"(æè¿°{len(job_description)}å­—)"
                        )

                        browser_page.wait_for_timeout(500)

                    except Exception as e:
                        pbar.write(f"âŒ å¤„ç†èŒä½æ—¶å‡ºé”™: {str(e)}")

                # å¦‚æœå·²ç»æŠ“å–åˆ°è¶³å¤Ÿçš„å²—ä½ï¼Œé€€å‡ºå¾ªç¯
                if valid_count >= max_count:
                    break

                # å°è¯•ç¿»é¡µ
                try:
                    # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
                    next_button = browser_page.locator(".options-pages a.next")

                    # æ£€æŸ¥ä¸‹ä¸€é¡µæŒ‰é’®æ˜¯å¦å­˜åœ¨ä¸”å¯ç‚¹å‡»
                    if next_button.count() > 0:
                        # æ£€æŸ¥æ˜¯å¦è¢«ç¦ç”¨
                        is_disabled = next_button.get_attribute("class")
                        if is_disabled and "disabled" in is_disabled:
                            pbar.write("ğŸ“„ å·²åˆ°æœ€åä¸€é¡µï¼Œæ— æ³•ç»§ç»­ç¿»é¡µ")
                            break

                        pbar.write(f"ğŸ“„ æ­£åœ¨ç¿»åˆ°ç¬¬{page_num + 1}é¡µ...")
                        next_button.click()
                        browser_page.wait_for_timeout(2000)
                        page_num += 1
                    else:
                        pbar.write("ğŸ“„ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œå¯èƒ½å·²åˆ°æœ€åä¸€é¡µ")
                        break

                except Exception as e:
                    pbar.write(f"âš ï¸ ç¿»é¡µå¤±è´¥: {str(e)}")
                    break

            # å†™å…¥æ‰€æœ‰æ•°æ®
            if jobs_buffer:
                with open(output_path, "a", encoding="utf-8-sig", newline="") as f:
                    writer = csv.DictWriter(f, fieldnames=jobs_buffer[0].keys())
                    writer.writerows(jobs_buffer)
                print(f"\nğŸ’¾ å·²å†™å…¥ {len(jobs_buffer)} æ¡èŒä½æ•°æ®")
                print(f"ğŸ“Š ç»Ÿè®¡: æœ‰æ•ˆå²—ä½ {valid_count} ä¸ªï¼Œè¿‡æ»¤å²—ä½ {filtered_count + filtered_english_count} ä¸ª")
                print(f"   - å­—æ•°ä¸è¶³è¿‡æ»¤: {filtered_count} ä¸ª")
                print(f"   - è‹±æ–‡JDè¿‡æ»¤: {filtered_english_count} ä¸ª")
            else:
                print("\nâš ï¸ æœªæŠ“å–åˆ°ä»»ä½•èŒä½æ•°æ®")

        finally:
            # å…³é—­è¿›åº¦æ¡
            if pbar is not None:
                try:
                    pbar.close()
                except Exception:
                    pass
            # ç¡®ä¿æµè§ˆå™¨å…³é—­
            browser.close()

    print(f"âœ… å·²è·å–èŒä½æ•°æ®ï¼Œä¿å­˜è·¯å¾„: {output_path}")
    return output_path


# è§£æå“åº”çš„è¾…åŠ©å‡½æ•°
def _parse_response(response, captured_responses):
    try:
        data = response.json()
        captured_responses.append(data)
    except Exception as e:
        print(f"è§£æå“åº”å¤±è´¥: {e}")


def parse_job_names(raw_jobs: str) -> list[str]:
    if not raw_jobs:
        return list(job_dict.keys())

    names = [name.strip() for name in raw_jobs.split(",") if name.strip()]
    unknown = [name for name in names if name not in job_dict]
    if unknown:
        raise ValueError(f"Unknown job names: {', '.join(unknown)}")

    return names


def append_csv_to_jsonl(csv_path: Path, combined_file, job_name: str, job_code: str) -> int:
    count = 0
    with open(csv_path, "r", encoding="utf-8-sig", newline="") as handle:
        reader = csv.DictReader(handle)
        for row in reader:
            row["job_category"] = job_name
            row["job_code"] = job_code
            combined_file.write(json.dumps(row, ensure_ascii=False) + "\n")
            count += 1
    return count


async def crawl_jobs(
    job_names: list[str],
    output_dir: Path,
    combined_path: Path,
    max_count: int,
    min_description_length: int,
    city: str,
    job_type: str,
    append: bool,
) -> None:
    mode = "a" if append else "w"
    with open(combined_path, mode, encoding="utf-8") as combined_file:
        for job_name in job_names:
            job_code = job_dict[job_name]
            print(f"\n=== Crawling {job_name} ({job_code}) ===")

            url = get_job_url({"job": job_name}, city=city, jobType=job_type)
            job_output_dir = output_dir / job_code

            try:
                csv_path = await get_job_details(
                    url=url,
                    max_count=max_count,
                    min_description_length=min_description_length,
                    output_dir=str(job_output_dir),
                )
            except Exception as exc:
                print(f"Failed to crawl {job_name}: {exc}")
                continue

            if not csv_path or not Path(csv_path).exists():
                print(f"No output generated for {job_name}")
                continue

            appended = append_csv_to_jsonl(Path(csv_path), combined_file, job_name, job_code)
            print(f"Added {appended} rows to {combined_path}")


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Offline crawl jobs and build a JSONL dataset.")
    parser.add_argument("--jobs", help="Comma-separated job names from tools/mappings.py")
    parser.add_argument("--max-count", type=int, default=50)
    parser.add_argument("--min-description-length", type=int, default=200)
    parser.add_argument("--city", default="100010000")
    parser.add_argument("--job-type", default="1901")
    parser.add_argument("--output-dir", default=str(DEFAULT_OUTPUT_DIR))
    parser.add_argument("--combined-path", default=str(DEFAULT_COMBINED_PATH))
    parser.add_argument("--append", action="store_true", help="Append to combined JSONL instead of overwrite")
    return parser


def main() -> None:
    parser = build_parser()
    args = parser.parse_args()

    os.environ.setdefault("JOB_CRAWL_USER_DATA_DIR", str(ROOT_DIR / "backend" / "user_data"))

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    combined_path = Path(args.combined_path)
    combined_path.parent.mkdir(parents=True, exist_ok=True)

    job_names = parse_job_names(args.jobs)

    crawl_jobs(
        job_names=job_names,
        output_dir=output_dir,
        combined_path=combined_path,
        max_count=args.max_count,
        min_description_length=args.min_description_length,
        city=args.city,
        job_type=args.job_type,
        append=args.append,
    )


if __name__ == "__main__":
    main()
